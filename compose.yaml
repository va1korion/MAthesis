services:
  llama-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: llama-server
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    command: >
      --model /models/YandexGPT_GGUF/yandexgpt-5-lite-8b-instruct-q4_k_m.gguf
      --port 8000
      --predict -2
      --temp 0.1
      --dry-multiplier 0.8
      --dry-base 1.75
      --ctx-size 32768
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
  fastapi-app:
      build: ./src
      container_name: fastapi-app
      ports:
        - "8080:80"
      depends_on:
        - llama-server
      environment:
        - LLAMA_SERVER_URL=http://llama-server:8000
        - ITMO_CLIENT_ID=${ITMO_CLIENT_ID}
        - ITMO_CLIENT_SECRET=${ITMO_CLIENT_SECRET}
        - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
      volumes:
        - ./data:/app/data
        - ./chroma_langchain_db:/app/chroma_langchain_db