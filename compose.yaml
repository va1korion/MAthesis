services:
  llama-server:
    privileged: true
    image: docker.io/local/llama.cpp:server-cuda
    container_name: llama-server
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    command: >
      --model /models/YandexGPT_GGUF/yandexgpt-5-lite-8b-instruct-q4_k_m.gguf
      --port 8000
      --predict -2
      --temp 0.1
      --dry-multiplier 0.8
      --dry-base 1.75
      --ctx-size 32768
      --n_gpu_layers 32
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VERSION=12.8
  fastapi-app:
    build: ./src
    container_name: fastapi-app
    ports:
        - "8080:8000"
    depends_on:
        - llama-server
    environment:
        - LLAMA_SERVER_URL=http://llama-server:8000
        - ITMO_CLIENT_ID=${ITMO_CLIENT_ID}
        - ITMO_CLIENT_SECRET=${ITMO_CLIENT_SECRET}
        - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    volumes:
        - ./data:/data
        - ./chroma_langchain_db:/chroma_langchain_db